default:
  tags:
    - infra

stages:
  - init
  - validate
  - plan
  - apply-staging
  - approval
  - apply-prod

variables:
  STAGING_TF_VAR_FILE: terraform-staging.tfvars
  PRODUCTION_TF_VAR_FILE: terraform-production.tfvars
  TF_ROOT: ${CI_PROJECT_DIR}/terraform

# Common setup that runs before all jobs
.terraform-setup:
  image:
    name: hashicorp/terraform:1.11.4
    entrypoint: [""]
  before_script:
    - apk add --no-cache bash jq curl sed
    - terraform --version
    # Create backend_config.hcl from the example
    - |
      echo "Creating backend_config.hcl from example file"
      if [ -f "backend_config.hcl.example" ]; then
        # Copy the example file as a starting point
        cp backend_config.hcl.example backend_config.hcl
        
        # Update the bucket name from environment variable
        if [ -n "${AWS_STATE_BUCKET}" ]; then
          sed -i "s|bucket         = \"your-terraform-state-bucket\"|bucket         = \"${AWS_STATE_BUCKET}\"|g" backend_config.hcl
        else
          echo "WARNING: AWS_STATE_BUCKET environment variable not set, using default from example"
        fi
        
        # Update region if AWS_DEFAULT_REGION is set
        if [ -n "${AWS_DEFAULT_REGION}" ]; then
          sed -i "s|region         = \"eu-north-1\"|region         = \"${AWS_DEFAULT_REGION}\"|g" backend_config.hcl
        fi
        
        # Update key with environment-specific path
        sed -i "s|key            = \"terraform/state\"|key            = \"terraform/${CI_PROJECT_NAME}/${CI_ENVIRONMENT_NAME:-default}/terraform.tfstate\"|g" backend_config.hcl
        
        # Keep use_lockfile and encrypt as they are (true)
      else
        echo "ERROR: backend_config.hcl.example not found! Cannot create configuration."
        exit 1
      fi

    - echo "Content of generated backend_config.hcl:"
    - cat backend_config.hcl

    - echo "Creating tfvars files dynamically from environment variables..."

    # Fix for JSON arrays that come in as strings with escaped quotes
    - |
      process_json_array() {
        # Remove the outer quotes and unescape the inner quotes
        echo "$1" | sed 's/^"\(.*\)"$/\1/' | sed 's/\\"/"/g'
      }

    # Create staging tfvars file
    - |
      cat > ${STAGING_TF_VAR_FILE} <<EOF
      aws_region         = "${AWS_DEFAULT_REGION}"
      environment        = "staging"
      cli_admin_username = "${TF_VAR_cli_admin_username}"

      # VPC Configuration
      vpc_cidr             = "${TF_VAR_STAGING_vpc_cidr}"
      public_subnet_cidrs  = $(process_json_array "${TF_VAR_STAGING_public_subnet_cidrs}")
      private_subnet_cidrs = $(process_json_array "${TF_VAR_STAGING_private_subnet_cidrs}")
      availability_zones   = $(process_json_array "${TF_VAR_STAGING_availability_zones}")

      # EKS Configuration
      cluster_name    = "${TF_VAR_STAGING_cluster_name}"
      cluster_version = "${TF_VAR_STAGING_cluster_version}"

      # Node Group Configuration
      node_instance_types = $(process_json_array "${TF_VAR_STAGING_node_instance_types}")
      capacity_type       = "${TF_VAR_STAGING_capacity_type}"
      desired_capacity    = ${TF_VAR_STAGING_desired_capacity}
      min_capacity        = ${TF_VAR_STAGING_min_capacity}
      max_capacity        = ${TF_VAR_STAGING_max_capacity}
      EOF

    # Create production tfvars file
    - |
      cat > ${PRODUCTION_TF_VAR_FILE} <<EOF
      aws_region         = "${AWS_DEFAULT_REGION}"
      environment        = "production"
      cli_admin_username = "${TF_VAR_cli_admin_username}"

      # VPC Configuration
      vpc_cidr             = "${TF_VAR_PROD_vpc_cidr}"
      public_subnet_cidrs  = $(process_json_array "${TF_VAR_PROD_public_subnet_cidrs}")
      private_subnet_cidrs = $(process_json_array "${TF_VAR_PROD_private_subnet_cidrs}")
      availability_zones   = $(process_json_array "${TF_VAR_PROD_availability_zones}")

      # EKS Configuration
      cluster_name    = "${TF_VAR_PROD_cluster_name}"
      cluster_version = "${TF_VAR_PROD_cluster_version}"

      # Node Group Configuration
      node_instance_types = $(process_json_array "${TF_VAR_PROD_node_instance_types}")
      capacity_type       = "${TF_VAR_PROD_capacity_type}"
      desired_capacity    = ${TF_VAR_PROD_desired_capacity}
      min_capacity        = ${TF_VAR_PROD_min_capacity}
      max_capacity        = ${TF_VAR_PROD_max_capacity}
      EOF

    - echo "Running terraform init with backend config"
    - terraform init -backend-config=backend_config.hcl

# Initialize Terraform
init:
  stage: init
  extends: .terraform-setup
  script:
    - echo "Initializing Terraform..."
    - terraform state list || echo "No state exists yet or not accessible. If it is a new project, this is expected."
  artifacts:
    paths:
      - .terraform.lock.hcl
      - backend_config.hcl
      - ${STAGING_TF_VAR_FILE}
      - ${PRODUCTION_TF_VAR_FILE}
    expire_in: 1 week

# Validate Terraform configuration
validate:
  stage: validate
  extends: .terraform-setup
  script:
    - echo "Validating Terraform files..."
    - terraform fmt -check -recursive
    - terraform validate
    # Custom check for security best practices (TBA)
    - echo "Checking for hardcoded secrets..."
    - find . -type f -name "*.tf" -exec grep -l "access_key\|secret_key\|password\|token" {} \; | tee potential_secrets.txt
    - |
      if [ -s potential_secrets.txt ]; then 
        echo "WARNING: Potential hardcoded secrets found!"
      fi
  artifacts:
    paths:
      - potential_secrets.txt
      - backend_config.hcl
    when: on_failure
    expire_in: 1 week

# Plan changes for staging
plan-staging:
  stage: plan
  extends: .terraform-setup
  script:
    - echo "Planning changes for staging environment..."
    - echo "Checking AWS credential info..."
    - aws sts get-caller-identity || echo "Failed to get AWS identity"
    - terraform workspace select staging || terraform workspace new staging
    - |
      if [ -f "${STAGING_TF_VAR_FILE}" ]; then
        echo "Using tfvars file: ${STAGING_TF_VAR_FILE}"
        terraform plan -var-file=${STAGING_TF_VAR_FILE} -out=staging.tfplan
      else
        echo "Warning: ${STAGING_TF_VAR_FILE} not found, using default variables"
        terraform plan -out=staging.tfplan
      fi
    # Create a an output of the plan
    - terraform show -no-color staging.tfplan > staging-plan-output.txt
  artifacts:
    paths:
      - backend_config.hcl
      - staging.tfplan
      - staging-plan-output.txt
    expire_in: 1 week
  environment:
    name: staging
  dependencies:
    - init

# Apply changes to staging
apply-staging:
  stage: apply-staging
  extends: .terraform-setup
  dependencies:
    - plan-staging
  script:
    - echo "Applying changes to staging environment"
    - terraform workspace select staging || terraform workspace new staging
    - terraform apply -auto-approve staging.tfplan
    - terraform output -json > terraform-output-staging.json

    # Install kubectl to apply the generated Ingress manifest
    - echo "Installing kubectl to apply Ingress manifest"
    - apk add --no-cache curl
    - curl -LO "https://dl.k8s.io/release/v1.29.4/bin/linux/amd64/kubectl"
    - chmod +x kubectl && mv kubectl /usr/local/bin/

    # Install AWS CLI using Alpine's package manager
    - echo "Installing AWS CLI"
    - apk add --no-cache aws-cli
    - aws --version

    # Configure kubectl using AWS credentials and environment variables
    - |
      echo "Configuring kubectl for EKS cluster"
      # Use environment variable directly instead of terraform output
      CLUSTER_NAME="${TF_VAR_STAGING_cluster_name}"
      echo "Using EKS cluster name: $CLUSTER_NAME"
      aws eks update-kubeconfig --name $CLUSTER_NAME --region ${AWS_DEFAULT_REGION}

    # Create the staging namespace if it doesn't exist
    - |
      echo "Ensuring staging namespace exists..."
      kubectl get namespace staging || kubectl create namespace staging
      echo "Staging namespace is ready"

    # Apply the generated Ingress manifest
    - |
      echo "Applying generated API Gateway Ingress manifest to staging environment"
      PROCESSED_INGRESS_PATH="../manifests/networking/api-gateway-ingress.yaml"
      if [ -f "$PROCESSED_INGRESS_PATH" ]; then
        echo "Found processed Ingress manifest at $PROCESSED_INGRESS_PATH"
        kubectl apply -f $PROCESSED_INGRESS_PATH -n staging
        echo "Ingress applied successfully!"
      else
        echo "ERROR: Processed Ingress manifest not found at expected path: $PROCESSED_INGRESS_PATH"
        # Try to find it using the terraform output if available
        INGRESS_PATH=$(terraform output -raw api_gateway_ingress_path 2>/dev/null || echo "")
        if [ -n "$INGRESS_PATH" ] && [ -f "$INGRESS_PATH" ]; then
          echo "Found Ingress manifest using terraform output at $INGRESS_PATH"
          kubectl apply -f $INGRESS_PATH -n staging
          echo "Ingress applied successfully!!!"
        else
          echo "WARNING: Could not find processed Ingress manifest. Ingress not applied."
        fi
      fi
  artifacts:
    paths:
      - terraform-output-staging.json
      - backend_config.hcl
    expire_in: 1 week
  environment:
    name: staging
    on_stop: cleanup-staging
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: manual
      allow_failure: true

# Manual approval for production deployment
approval-prod:
  stage: approval
  image: alpine:latest
  script:
    - echo "Deployment to production requires approval"
    - echo "Review the staging environment before proceeding"
    - echo "After approval, changes will be directly applied to production"
  environment:
    name: production-approval
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: manual
      allow_failure: true

# Apply changes to production (includes planning)
apply-prod:
  stage: apply-prod
  extends: .terraform-setup
  dependencies:
    - init
    - validate
    - approval-prod
  script:
    - echo "Planning and applying changes to production environment"
    - terraform workspace select production || terraform workspace new production
    - |
      if [ -f "${PRODUCTION_TF_VAR_FILE}" ]; then
        echo "Using tfvars file: ${PRODUCTION_TF_VAR_FILE}"
        terraform plan -var-file=${PRODUCTION_TF_VAR_FILE} -out=production.tfplan
      else
        echo "Warning: ${PRODUCTION_TF_VAR_FILE} not found, using default variables"
        terraform plan -out=production.tfplan
      fi
    - terraform apply -auto-approve production.tfplan
    - terraform output -json > terraform-output-production.json

    # Install kubectl to apply the generated Ingress manifest
    - echo "Installing kubectl to apply Ingress manifest"
    - apk add --no-cache curl
    - curl -LO "https://dl.k8s.io/release/v1.29.4/bin/linux/amd64/kubectl"
    - chmod +x kubectl && mv kubectl /usr/local/bin/

    # Install AWS CLI using Alpine's package manager
    - echo "Installing AWS CLI"
    - apk add --no-cache aws-cli
    - aws --version

    # Configure kubectl using AWS credentials and environment variables
    - |
      echo "Configuring kubectl for EKS cluster"
      # Use environment variable directly instead of terraform output
      CLUSTER_NAME="${TF_VAR_PROD_cluster_name}"
      echo "Using EKS cluster name: $CLUSTER_NAME"
      aws eks update-kubeconfig --name $CLUSTER_NAME --region ${AWS_DEFAULT_REGION}

    # Create the production namespace if it doesn't exist
    - |
      echo "Ensuring production namespace exists..."
      kubectl get namespace production || kubectl create namespace production
      echo "Production namespace is ready"

    # Apply the generated Ingress manifest
    - |
      echo "Applying generated API Gateway Ingress manifest to production environment"
      PROCESSED_INGRESS_PATH="../manifests/networking/api-gateway-ingress.yaml"
      if [ -f "$PROCESSED_INGRESS_PATH" ]; then
        echo "Found processed Ingress manifest at $PROCESSED_INGRESS_PATH"
        kubectl apply -f $PROCESSED_INGRESS_PATH -n production
        echo "Ingress applied successfully!"
      else
        echo "ERROR: Processed Ingress manifest not found at expected path: $PROCESSED_INGRESS_PATH"
        # Try to find it using the terraform output if available
        INGRESS_PATH=$(terraform output -raw api_gateway_ingress_path 2>/dev/null || echo "")
        if [ -n "$INGRESS_PATH" ] && [ -f "$INGRESS_PATH" ]; then
          echo "Found Ingress manifest using terraform output at $INGRESS_PATH"
          kubectl apply -f $INGRESS_PATH -n production
          echo "Ingress applied successfully!"
        else
          echo "WARNING: Could not find processed Ingress manifest. Ingress not applied."
        fi
      fi

    # Tagging successful deployments
    - |
      if [ -n "${CI_COMMIT_TAG}" ]; then
        echo "This build is tagged as ${CI_COMMIT_TAG}"
      else
        CI_DEPLOY_VERSION=$(date '+%Y%m%d%H%M%S')
        echo "Creating deployment tag v${CI_DEPLOY_VERSION}"
        echo "${CI_DEPLOY_VERSION}" > deploy-version.txt
      fi
  artifacts:
    paths:
      - terraform-output-production.json
      - backend_config.hcl
      - deploy-version.txt
    expire_in: 4 weeks
  environment:
    name: production
    on_stop: cleanup-prod
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: manual
      allow_failure: true

# Optional cleanup jobs (only triggered manually)
cleanup-staging:
  stage: apply-staging
  extends: .terraform-setup
  script:
    - |
      echo "WARNING: This will DESTROY all resources in the staging environment!!!"

    # Install kubectl and AWS CLI to clean up Kubernetes resources
    - echo "Installing kubectl and AWS CLI for Kubernetes cleanup"
    - apk add --no-cache curl aws-cli
    - curl -LO "https://dl.k8s.io/release/v1.29.4/bin/linux/amd64/kubectl"
    - chmod +x kubectl && mv kubectl /usr/local/bin/
    - kubectl version --client

    # Configure kubectl for the staging cluster
    - |
      echo "Configuring kubectl for staging EKS cluster"
      CLUSTER_NAME="${TF_VAR_STAGING_cluster_name}"
      echo "Using EKS cluster name: $CLUSTER_NAME"
      aws eks update-kubeconfig --name $CLUSTER_NAME --region ${AWS_DEFAULT_REGION}

    # Clean up all resources in the staging namespace
    - |
      echo "Cleaning up Kubernetes resources in staging namespace..."
      # First check if namespace exists
      if kubectl get namespace staging &>/dev/null; then
        echo "Deleting all deployments in staging namespace"
        kubectl delete deployments --all -n staging --timeout=300s --wait=true || true
        
        echo "Deleting all statefulsets in staging namespace"
        kubectl delete statefulsets --all -n staging --timeout=300s --wait=true || true
        
        echo "Deleting all services in staging namespace"
        kubectl delete services --all -n staging --timeout=300s --wait=true || true
        
        echo "Deleting all ingresses in staging namespace"
        kubectl delete ingress --all -n staging --timeout=300s --wait=true || true
        
        echo "Deleting all configmaps in staging namespace"
        kubectl delete configmaps --all -n staging --timeout=300s --wait=true || true
        
        echo "Deleting all secrets in staging namespace"
        kubectl delete secrets --all -n staging --timeout=300s --wait=true || true
        
        echo "Deleting all persistent volume claims in staging namespace"
        kubectl delete pvc --all -n staging --timeout=300s --wait=true || true
        
        echo "Deleting all persistent volumes associated with staging namespace"
        kubectl get pv -o json | jq -r '.items[] | select(.spec.claimRef.namespace == "staging") | .metadata.name' | xargs -I{} kubectl delete pv {} --timeout=300s || true
        
        echo "Kubernetes resources cleanup completed for staging namespace"
      else
        echo "Staging namespace not found, skipping Kubernetes resources cleanup"
      fi

    # Now run terraform destroy
    - terraform workspace select staging || terraform workspace new staging
    - |
      if [ -f "${STAGING_TF_VAR_FILE}" ]; then
        terraform destroy -var-file=${STAGING_TF_VAR_FILE} -auto-approve
      else
        terraform destroy -auto-approve
      fi
  environment:
    name: staging
    action: stop
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: manual
      allow_failure: true

cleanup-prod:
  stage: apply-prod
  extends: .terraform-setup
  script:
    - |
      echo "WARNING: This will DESTROY all resources in the production environment"

    # Install kubectl and AWS CLI to clean up Kubernetes resources
    - echo "Installing kubectl and AWS CLI for Kubernetes cleanup"
    - apk add --no-cache curl aws-cli
    - curl -LO "https://dl.k8s.io/release/v1.29.4/bin/linux/amd64/kubectl"
    - chmod +x kubectl && mv kubectl /usr/local/bin/
    - kubectl version --client

    # Configure kubectl for the production cluster
    - |
      echo "Configuring kubectl for production EKS cluster"
      CLUSTER_NAME="${TF_VAR_PROD_cluster_name}"
      echo "Using EKS cluster name: $CLUSTER_NAME"
      aws eks update-kubeconfig --name $CLUSTER_NAME --region ${AWS_DEFAULT_REGION}

    # Clean up all resources in the production namespace
    - |
      echo "Cleaning up Kubernetes resources in production namespace"
      # First check if namespace exists
      if kubectl get namespace production &>/dev/null; then
        echo "Deleting all deployments in production namespace"
        kubectl delete deployments --all -n production --timeout=300s --wait=true || true
        
        echo "Deleting all statefulsets in production namespace"
        kubectl delete statefulsets --all -n production --timeout=300s --wait=true || true
        
        echo "Deleting all services in production namespace"
        kubectl delete services --all -n production --timeout=300s --wait=true || true
        
        echo "Deleting all ingresses in production namespace"
        kubectl delete ingress --all -n production --timeout=300s --wait=true || true
        
        echo "Deleting all configmaps in production namespace"
        kubectl delete configmaps --all -n production --timeout=300s --wait=true || true
        
        echo "Deleting all secrets in production namespace"
        kubectl delete secrets --all -n production --timeout=300s --wait=true || true
        
        echo "Deleting all persistent volume claims in production namespace"
        kubectl delete pvc --all -n production --timeout=300s --wait=true || true
        
        echo "Deleting all persistent volumes associated with production namespace"
        kubectl get pv -o json | jq -r '.items[] | select(.spec.claimRef.namespace == "production") | .metadata.name' | xargs -I{} kubectl delete pv {} --timeout=300s || true
        
        echo "Kubernetes resources cleanup completed for production namespace"
      else
        echo "Production namespace not found, skipping Kubernetes resources cleanup"
      fi

    # Now run terraform destroy
    - terraform workspace select production || terraform workspace new production
    - |
      if [ -f "${PRODUCTION_TF_VAR_FILE}" ]; then
        terraform destroy -var-file=${PRODUCTION_TF_VAR_FILE} -auto-approve
      else
        terraform destroy -auto-approve
      fi
  environment:
    name: production
    action: stop
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: manual
      allow_failure: true
