default:
  tags:
    - infra

stages:
  - init
  - validate
  - plan
  - apply-staging
  - approval
  - apply-prod

variables:
  STAGING_TF_VAR_FILE: terraform-staging.tfvars
  PRODUCTION_TF_VAR_FILE: terraform-production.tfvars

# Common setup that runs before all jobs
.terraform-setup:
  image:
    name: hashicorp/terraform:1.11.4
    entrypoint: [""]
  before_script:
    - apk add --no-cache bash jq curl sed
    - terraform --version
    # Create backend_config.hcl from the example
    - |
      echo "Creating backend_config.hcl from example file..."
      if [ -f "backend_config.hcl.example" ]; then
        # Copy the example file as a starting point
        cp backend_config.hcl.example backend_config.hcl
        
        # Update the bucket name and region from environment variables
        [ -n "${AWS_STATE_BUCKET}" ] && sed -i "s|bucket         = \"your-terraform-state-bucket\"|bucket         = \"${AWS_STATE_BUCKET}\"|g" backend_config.hcl || echo "WARNING: AWS_STATE_BUCKET not set"
        [ -n "${AWS_DEFAULT_REGION}" ] && sed -i "s|region         = \"eu-north-1\"|region         = \"${AWS_DEFAULT_REGION}\"|g" backend_config.hcl
        
        # Update key with environment-specific path
        sed -i "s|key            = \"terraform/state\"|key            = \"terraform/${CI_PROJECT_NAME}/${CI_ENVIRONMENT_NAME:-default}/terraform.tfstate\"|g" backend_config.hcl
      else
        echo "ERROR: backend_config.hcl.example not found! Cannot create configuration."
        exit 1
      fi

    # Process JSON arrays function
    - |
      process_json_array() {
        echo "$1" | sed 's/^"\(.*\)"$/\1/' | sed 's/\\"/"/g'
      }

    # Create environment tfvars files
    - |
      # Create staging tfvars file
      cat > ${STAGING_TF_VAR_FILE} <<EOF
      aws_region         = "${AWS_DEFAULT_REGION}"
      environment        = "staging"
      cli_admin_username = "${TF_VAR_cli_admin_username}"

      # VPC Configuration
      vpc_cidr             = "${TF_VAR_STAGING_vpc_cidr}"
      public_subnet_cidrs  = $(process_json_array "${TF_VAR_STAGING_public_subnet_cidrs}")
      private_subnet_cidrs = $(process_json_array "${TF_VAR_STAGING_private_subnet_cidrs}")
      availability_zones   = $(process_json_array "${TF_VAR_STAGING_availability_zones}")

      # EKS Configuration
      cluster_name    = "${TF_VAR_STAGING_cluster_name}"
      cluster_version = "${TF_VAR_STAGING_cluster_version}"

      # Node Group Configuration
      node_instance_types = $(process_json_array "${TF_VAR_STAGING_node_instance_types}")
      capacity_type       = "${TF_VAR_STAGING_capacity_type}"
      desired_capacity    = ${TF_VAR_STAGING_desired_capacity}
      min_capacity        = ${TF_VAR_STAGING_min_capacity}
      max_capacity        = ${TF_VAR_STAGING_max_capacity}
      EOF

      # Create production tfvars file
      cat > ${PRODUCTION_TF_VAR_FILE} <<EOF
      aws_region         = "${AWS_DEFAULT_REGION}"
      environment        = "production"
      cli_admin_username = "${TF_VAR_cli_admin_username}"

      # VPC Configuration
      vpc_cidr             = "${TF_VAR_PROD_vpc_cidr}"
      public_subnet_cidrs  = $(process_json_array "${TF_VAR_PROD_public_subnet_cidrs}")
      private_subnet_cidrs = $(process_json_array "${TF_VAR_PROD_private_subnet_cidrs}")
      availability_zones   = $(process_json_array "${TF_VAR_PROD_availability_zones}")

      # EKS Configuration
      cluster_name    = "${TF_VAR_PROD_cluster_name}"
      cluster_version = "${TF_VAR_PROD_cluster_version}"

      # Node Group Configuration
      node_instance_types = $(process_json_array "${TF_VAR_PROD_node_instance_types}")
      capacity_type       = "${TF_VAR_PROD_capacity_type}"
      desired_capacity    = ${TF_VAR_PROD_desired_capacity}
      min_capacity        = ${TF_VAR_PROD_min_capacity}
      max_capacity        = ${TF_VAR_PROD_max_capacity}
      EOF

    - terraform init -backend-config=backend_config.hcl

# Initialize Terraform
init:
  stage: init
  extends: .terraform-setup
  script:
    - echo "Initializing Terraform..."
    - terraform state list || echo "No state exists yet or not accessible. If it is a new project, this is expected."
  artifacts:
    paths:
      - .terraform.lock.hcl
      - backend_config.hcl
      - ${STAGING_TF_VAR_FILE}
      - ${PRODUCTION_TF_VAR_FILE}
    expire_in: 1 week

# Validate Terraform configuration
validate:
  stage: validate
  extends: .terraform-setup
  script:
    - echo "Validating Terraform files..."
    - terraform fmt -check -recursive
    - terraform validate
    # Custom check for security best practices (TBA)
    - echo "Checking for hardcoded secrets..."
    - find . -type f -name "*.tf" -exec grep -l "access_key\|secret_key\|password\|token" {} \; | tee potential_secrets.txt
    - |
      if [ -s potential_secrets.txt ]; then 
        echo "WARNING: Potential hardcoded secrets found!"
      fi
  artifacts:
    paths:
      - potential_secrets.txt
      - backend_config.hcl
    when: on_failure
    expire_in: 1 week

# Plan changes for staging
plan-staging:
  stage: plan
  extends: .terraform-setup
  script:
    - echo "Planning changes for staging environment..."
    - echo "Checking AWS credential info"
    - aws sts get-caller-identity || echo "Failed to get AWS identity"
    - terraform workspace select staging || terraform workspace new staging
    - |
      if [ -f "${STAGING_TF_VAR_FILE}" ]; then
        echo "Using tfvars file: ${STAGING_TF_VAR_FILE}"
        terraform plan -var-file=${STAGING_TF_VAR_FILE} -out=staging.tfplan
      else
        echo "Warning: ${STAGING_TF_VAR_FILE} not found, using default variables"
        terraform plan -out=staging.tfplan
      fi
    # Create a an output of the plan
    - terraform show -no-color staging.tfplan > staging-plan-output.txt
  artifacts:
    paths:
      - backend_config.hcl
      - staging.tfplan
      - staging-plan-output.txt
    expire_in: 1 week
  environment:
    name: staging
  dependencies:
    - init

# Apply changes to staging
apply-staging:
  stage: apply-staging
  extends: .terraform-setup
  dependencies:
    - plan-staging
  variables:
    ENVIRONMENT: staging
    CLUSTER_NAME: ${TF_VAR_STAGING_cluster_name}
  script:
    - echo "Applying changes to staging environment"
    - terraform workspace select staging || terraform workspace new staging
    - terraform apply -auto-approve staging.tfplan
    - terraform output -json > terraform-output-staging.json

    # Setup Kubernetes tools and configure kubectl
    - echo "Installing kubectl and AWS CLI"
    - apk add --no-cache curl aws-cli
    - curl -LO "https://dl.k8s.io/release/v1.29.4/bin/linux/amd64/kubectl"
    - chmod +x kubectl && mv kubectl /usr/local/bin/

    # Configure kubectl
    - |
      echo "Configuring kubectl for EKS cluster: $CLUSTER_NAME"
      aws eks update-kubeconfig --name $CLUSTER_NAME --region ${AWS_DEFAULT_REGION}

    # Create namespace if it doesn't exist
    - |
      echo "Ensuring ${ENVIRONMENT} namespace exists..."
      kubectl get namespace ${ENVIRONMENT} || kubectl create namespace ${ENVIRONMENT}
      echo "${ENVIRONMENT} namespace is ready"

    # Apply ingress manifest
    - |
      echo "Applying generated API Gateway Ingress manifest to ${ENVIRONMENT} environment"
      PROCESSED_INGRESS_PATH="../manifests/networking/api-gateway-ingress.yaml"
      if [ -f "$PROCESSED_INGRESS_PATH" ]; then
        kubectl apply -f $PROCESSED_INGRESS_PATH -n ${ENVIRONMENT}
        echo "Ingress applied successfully!"
      else
        INGRESS_PATH=$(terraform output -raw api_gateway_ingress_path 2>/dev/null || echo "")
        if [ -n "$INGRESS_PATH" ] && [ -f "$INGRESS_PATH" ]; then
          kubectl apply -f $INGRESS_PATH -n ${ENVIRONMENT}
          echo "Ingress applied successfully!"
        else
          echo "WARNING: Could not find Ingress manifest. Ingress not applied."
        fi
      fi
  artifacts:
    paths:
      - terraform-output-staging.json
      - backend_config.hcl
    expire_in: 1 week
  environment:
    name: staging
    on_stop: cleanup-staging
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: manual
      allow_failure: true

# Manual approval for production deployment
approval-prod:
  stage: approval
  image: alpine:latest
  script:
    - echo "Deployment to production requires approval"
    - echo "Review the staging environment before proceeding"
    - echo "After approval, changes will be directly applied to production"
  environment:
    name: production-approval
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: manual
      allow_failure: true

# Apply changes to production (includes planning)
apply-prod:
  stage: apply-prod
  extends: .terraform-setup
  dependencies:
    - init
    - validate
    - approval-prod
  variables:
    ENVIRONMENT: production
    CLUSTER_NAME: ${TF_VAR_PROD_cluster_name}
  script:
    - echo "Planning and applying changes to production environment"
    - terraform workspace select production || terraform workspace new production
    - |
      if [ -f "${PRODUCTION_TF_VAR_FILE}" ]; then
        terraform plan -var-file=${PRODUCTION_TF_VAR_FILE} -out=production.tfplan
      else
        terraform plan -out=production.tfplan
      fi
    - terraform apply -auto-approve production.tfplan
    - terraform output -json > terraform-output-production.json

    # Setup Kubernetes tools and configure kubectl
    - echo "Installing kubectl and AWS CLI"
    - apk add --no-cache curl aws-cli
    - curl -LO "https://dl.k8s.io/release/v1.29.4/bin/linux/amd64/kubectl"
    - chmod +x kubectl && mv kubectl /usr/local/bin/

    # Configure kubectl
    - |
      echo "Configuring kubectl for EKS cluster: $CLUSTER_NAME"
      aws eks update-kubeconfig --name $CLUSTER_NAME --region ${AWS_DEFAULT_REGION}

    # Create namespace if it doesn't exist
    - |
      echo "Ensuring ${ENVIRONMENT} namespace exists..."
      kubectl get namespace ${ENVIRONMENT} || kubectl create namespace ${ENVIRONMENT}
      echo "${ENVIRONMENT} namespace is ready"

    # Apply ingress manifest
    - |
      echo "Applying generated API Gateway Ingress manifest to ${ENVIRONMENT} environment"
      PROCESSED_INGRESS_PATH="../manifests/networking/api-gateway-ingress.yaml"
      if [ -f "$PROCESSED_INGRESS_PATH" ]; then
        kubectl apply -f $PROCESSED_INGRESS_PATH -n ${ENVIRONMENT}
        echo "Ingress applied successfully!"
      else
        INGRESS_PATH=$(terraform output -raw api_gateway_ingress_path 2>/dev/null || echo "")
        if [ -n "$INGRESS_PATH" ] && [ -f "$INGRESS_PATH" ]; then
          kubectl apply -f $INGRESS_PATH -n ${ENVIRONMENT}
          echo "Ingress applied successfully!"
        else
          echo "WARNING: Could not find Ingress manifest. Ingress not applied."
        fi
      fi

    # Tagging successful deployments
    - |
      if [ -n "${CI_COMMIT_TAG}" ]; then
        echo "This build is tagged as ${CI_COMMIT_TAG}"
      else
        CI_DEPLOY_VERSION=$(date '+%Y%m%d%H%M%S')
        echo "Creating deployment tag v${CI_DEPLOY_VERSION}"
        echo "${CI_DEPLOY_VERSION}" > deploy-version.txt
      fi
  artifacts:
    paths:
      - terraform-output-production.json
      - backend_config.hcl
      - deploy-version.txt
    expire_in: 4 weeks
  environment:
    name: production
    on_stop: cleanup-prod
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: manual
      allow_failure: true

cleanup-staging:
  stage: apply-staging
  extends: .terraform-setup
  variables:
    ENVIRONMENT: staging
    CLUSTER_NAME: ${TF_VAR_STAGING_cluster_name}
    TF_VAR_FILE: ${STAGING_TF_VAR_FILE}
  script:
    - |
      echo "WARNING: This will DESTROY all resources in the ${ENVIRONMENT} environment"
    # Install kubectl and AWS CLI
    - apk add --no-cache curl aws-cli
    - curl -LO "https://dl.k8s.io/release/v1.29.4/bin/linux/amd64/kubectl"
    - chmod +x kubectl && mv kubectl /usr/local/bin/
    # Configure kubectl
    - |
      echo "Configuring kubectl for EKS cluster"
      aws eks update-kubeconfig --name $CLUSTER_NAME --region ${AWS_DEFAULT_REGION}
    # Clean up all resources in the namespace
    - |
      echo "Cleaning up Kubernetes resources in ${ENVIRONMENT} namespace"
      if kubectl get namespace ${ENVIRONMENT} &>/dev/null; then
        kubectl delete deployments --all -n ${ENVIRONMENT} --timeout=300s --wait=true || true
        kubectl delete statefulsets --all -n ${ENVIRONMENT} --timeout=300s --wait=true || true
        kubectl delete services --all -n ${ENVIRONMENT} --timeout=300s --wait=true || true
        kubectl delete ingress --all -n ${ENVIRONMENT} --timeout=300s --wait=true || true
        kubectl delete configmaps --all -n ${ENVIRONMENT} --timeout=300s --wait=true || true
        kubectl delete secrets --all -n ${ENVIRONMENT} --timeout=300s --wait=true || true
        kubectl delete pvc --all -n ${ENVIRONMENT} --timeout=300s --wait=true || true
        
        echo "Deleting persistent volumes associated with ${ENVIRONMENT} namespace"
        kubectl get pv -o json | jq -r ".items[] | select(.spec.claimRef.namespace == \"${ENVIRONMENT}\") | .metadata.name" | xargs -I{} kubectl delete pv {} --timeout=300s || true
        
        echo "Kubernetes resources cleanup completed for ${ENVIRONMENT} namespace"
      else
        echo "${ENVIRONMENT} namespace not found, skipping Kubernetes resources cleanup"
      fi
    # Run terraform destroy
    - terraform workspace select ${ENVIRONMENT} || terraform workspace new ${ENVIRONMENT}
    - |
      if [ -f "${TF_VAR_FILE}" ]; then
        terraform destroy -var-file=${TF_VAR_FILE} -auto-approve
      else
        terraform destroy -auto-approve
      fi
  environment:
    name: staging
    action: stop
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: manual
      allow_failure: true

cleanup-prod:
  stage: apply-prod
  extends: .terraform-setup
  variables:
    ENVIRONMENT: production
    CLUSTER_NAME: ${TF_VAR_PROD_cluster_name}
    TF_VAR_FILE: ${PRODUCTION_TF_VAR_FILE}
  script:
    - |
      echo "WARNING: This will DESTROY all resources in the ${ENVIRONMENT} environment."
    # Install kubectl and AWS CLI
    - apk add --no-cache curl aws-cli
    - curl -LO "https://dl.k8s.io/release/v1.29.4/bin/linux/amd64/kubectl"
    - chmod +x kubectl && mv kubectl /usr/local/bin/
    # Configure kubectl
    - |
      echo "Configuring kubectl for EKS cluster"
      aws eks update-kubeconfig --name $CLUSTER_NAME --region ${AWS_DEFAULT_REGION}
    # Clean up all resources in the namespace
    - |
      echo "Cleaning up Kubernetes resources in ${ENVIRONMENT} namespace"
      if kubectl get namespace ${ENVIRONMENT} &>/dev/null; then
        kubectl delete deployments --all -n ${ENVIRONMENT} --timeout=300s --wait=true || true
        kubectl delete statefulsets --all -n ${ENVIRONMENT} --timeout=300s --wait=true || true
        kubectl delete services --all -n ${ENVIRONMENT} --timeout=300s --wait=true || true
        kubectl delete ingress --all -n ${ENVIRONMENT} --timeout=300s --wait=true || true
        kubectl delete configmaps --all -n ${ENVIRONMENT} --timeout=300s --wait=true || true
        kubectl delete secrets --all -n ${ENVIRONMENT} --timeout=300s --wait=true || true
        kubectl delete pvc --all -n ${ENVIRONMENT} --timeout=300s --wait=true || true
        
        echo "Deleting persistent volumes associated with ${ENVIRONMENT} namespace"
        kubectl get pv -o json | jq -r ".items[] | select(.spec.claimRef.namespace == \"${ENVIRONMENT}\") | .metadata.name" | xargs -I{} kubectl delete pv {} --timeout=300s || true
        
        echo "Kubernetes resources cleanup completed for ${ENVIRONMENT} namespace"
      else
        echo "${ENVIRONMENT} namespace not found, skipping Kubernetes resources cleanup"
      fi
    # Run terraform destroy
    - terraform workspace select ${ENVIRONMENT} || terraform workspace new ${ENVIRONMENT}
    - |
      if [ -f "${TF_VAR_FILE}" ]; then
        terraform destroy -var-file=${TF_VAR_FILE} -auto-approve
      else
        terraform destroy -auto-approve
      fi
  environment:
    name: production
    action: stop
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: manual
      allow_failure: true
